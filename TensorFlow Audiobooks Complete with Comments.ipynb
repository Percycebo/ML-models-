{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical example. Audiobooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "You are given data from an Audiobook app. Logically, it relates only to the audio versions of books. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertizing to him/her. If we can focus our efforts ONLY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "You have a .csv summarizing the data. There are several variables: Customer ID, Book length in mins_avg (average of all purchases), Book length in minutes_sum (sum of all purchases), Price Paid_avg (average of all purchases), Price paid_sum (sum of all purchases), Review (a Boolean variable), Review (out of 10), Total minutes listened, Completion (from 0 to 1), Support requests (number), and Last visited minus purchase date (in days).\n",
    "\n",
    "So these are the inputs (excluding customer ID, as it is completely arbitrary. It's more like a name, than a number).\n",
    "\n",
    "The targets are a Boolean variable (so 0, or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information. \n",
    "\n",
    "The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again. \n",
    "\n",
    "This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the machine learning algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "# we extract the inputs using the keyword under which we saved them\n",
    "# to ensure that they are all floats, let's also take care of that\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "# we create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "Outline, optimizers, loss, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 11:58:41.925773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-27 11:58:41.926874: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-09-27 11:58:42.034956: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-09-27 11:58:42.283829: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 - 1s - loss: 0.6380 - accuracy: 0.6049 - val_loss: 0.5796 - val_accuracy: 0.6890 - 945ms/epoch - 26ms/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 11:58:42.944212: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 - 0s - loss: 0.5272 - accuracy: 0.7139 - val_loss: 0.5165 - val_accuracy: 0.7293 - 192ms/epoch - 5ms/step\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.4627 - accuracy: 0.7611 - val_loss: 0.4753 - val_accuracy: 0.7360 - 178ms/epoch - 5ms/step\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.4224 - accuracy: 0.7837 - val_loss: 0.4562 - val_accuracy: 0.7494 - 183ms/epoch - 5ms/step\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3985 - accuracy: 0.7896 - val_loss: 0.4367 - val_accuracy: 0.7472 - 175ms/epoch - 5ms/step\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3838 - accuracy: 0.7980 - val_loss: 0.4274 - val_accuracy: 0.7494 - 180ms/epoch - 5ms/step\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3729 - accuracy: 0.7980 - val_loss: 0.4178 - val_accuracy: 0.7651 - 173ms/epoch - 5ms/step\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3656 - accuracy: 0.8022 - val_loss: 0.4125 - val_accuracy: 0.7718 - 177ms/epoch - 5ms/step\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3594 - accuracy: 0.8027 - val_loss: 0.4079 - val_accuracy: 0.7718 - 206ms/epoch - 6ms/step\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3546 - accuracy: 0.8086 - val_loss: 0.4046 - val_accuracy: 0.7897 - 209ms/epoch - 6ms/step\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.3542 - accuracy: 0.7997 - val_loss: 0.4074 - val_accuracy: 0.7606 - 198ms/epoch - 5ms/step\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.3488 - accuracy: 0.8114 - val_loss: 0.4049 - val_accuracy: 0.7740 - 202ms/epoch - 6ms/step\n",
      "\n",
      "\n",
      "\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.3584 - accuracy: 0.8192\n",
      "\n",
      "Test loss: 0.36. Test accuracy: 81.92%\n"
     ]
    }
   ],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 15\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(50, activation='relu'), # 2nd hidden layer\n",
    "    #tf.keras.layers.Dense(hidden_layer_size, activation='relu',kernel_initializer='he_uniform'),\n",
    "    \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "#Choose the optimizer and the loss function\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Training\n",
    "# That's where we train the model we have built.\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(train_inputs,\n",
    "          train_targets, \n",
    "          batch_size=batch_size, \n",
    "          epochs=max_epochs, \n",
    "          callbacks=[early_stopping], \n",
    "          validation_data=(validation_inputs, validation_targets), \n",
    "          verbose = 2 # making sure we get enough information about the training process\n",
    "          )  \n",
    "\n",
    "print('\\n\\n')\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "# we create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 12:00:20.193243: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 1s - loss: 0.5250 - accuracy: 0.7220 - val_loss: 0.4725 - val_accuracy: 0.7315 - 558ms/epoch - 23ms/step\n",
      "Epoch 2/100\n",
      "24/24 - 0s - loss: 0.4079 - accuracy: 0.7840 - val_loss: 0.4226 - val_accuracy: 0.7696 - 133ms/epoch - 6ms/step\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 12:00:20.462037: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 - 0s - loss: 0.3753 - accuracy: 0.7879 - val_loss: 0.3994 - val_accuracy: 0.7875 - 138ms/epoch - 6ms/step\n",
      "Epoch 4/100\n",
      "24/24 - 0s - loss: 0.3558 - accuracy: 0.8097 - val_loss: 0.4025 - val_accuracy: 0.7875 - 128ms/epoch - 5ms/step\n",
      "Epoch 5/100\n",
      "24/24 - 0s - loss: 0.3487 - accuracy: 0.8128 - val_loss: 0.3795 - val_accuracy: 0.7808 - 128ms/epoch - 5ms/step\n",
      "Epoch 6/100\n",
      "24/24 - 0s - loss: 0.3435 - accuracy: 0.8058 - val_loss: 0.4147 - val_accuracy: 0.7897 - 128ms/epoch - 5ms/step\n",
      "Epoch 7/100\n",
      "24/24 - 0s - loss: 0.3416 - accuracy: 0.8178 - val_loss: 0.3762 - val_accuracy: 0.7740 - 135ms/epoch - 6ms/step\n",
      "Epoch 8/100\n",
      "24/24 - 0s - loss: 0.3391 - accuracy: 0.8136 - val_loss: 0.3814 - val_accuracy: 0.7785 - 127ms/epoch - 5ms/step\n",
      "Epoch 9/100\n",
      "24/24 - 0s - loss: 0.3298 - accuracy: 0.8215 - val_loss: 0.3683 - val_accuracy: 0.7964 - 126ms/epoch - 5ms/step\n",
      "Epoch 10/100\n",
      "24/24 - 0s - loss: 0.3294 - accuracy: 0.8203 - val_loss: 0.3666 - val_accuracy: 0.7987 - 126ms/epoch - 5ms/step\n",
      "Epoch 11/100\n",
      "24/24 - 0s - loss: 0.3323 - accuracy: 0.8173 - val_loss: 0.3887 - val_accuracy: 0.7852 - 127ms/epoch - 5ms/step\n",
      "Epoch 12/100\n",
      "24/24 - 0s - loss: 0.3237 - accuracy: 0.8229 - val_loss: 0.3682 - val_accuracy: 0.7964 - 134ms/epoch - 6ms/step\n",
      "\n",
      "\n",
      "\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.3488 - accuracy: 0.8125\n",
      "\n",
      "Test loss: 0.35. Test accuracy: 81.25%\n"
     ]
    }
   ],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "\n",
    "hidden_layer_size = 60\n",
    "    \n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    " \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "model2.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 150\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model2.fit(train_inputs,\n",
    "          train_targets, \n",
    "          batch_size=batch_size, \n",
    "          epochs=max_epochs, \n",
    "          callbacks=[early_stopping], \n",
    "          validation_data=(validation_inputs, validation_targets), \n",
    "          verbose = 2 # making sure we get enough information about the training process\n",
    "          )  \n",
    "\n",
    "print('\\n\\n')\n",
    "test_loss, test_accuracy = model2.evaluate(test_inputs, test_targets)\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "# we create 2 variables that will contain the test inputs and the test targets\n",
    "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 12:02:45.260434: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 - 1s - loss: 0.5120 - accuracy: 0.7153 - val_loss: 0.4160 - val_accuracy: 0.7673 - 988ms/epoch - 14ms/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 12:02:45.933486: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 - 1s - loss: 0.3752 - accuracy: 0.7907 - val_loss: 0.4073 - val_accuracy: 0.7450 - 555ms/epoch - 8ms/step\n",
      "Epoch 3/100\n",
      "72/72 - 1s - loss: 0.3520 - accuracy: 0.8069 - val_loss: 0.4022 - val_accuracy: 0.7673 - 530ms/epoch - 7ms/step\n",
      "Epoch 4/100\n",
      "72/72 - 0s - loss: 0.3413 - accuracy: 0.8069 - val_loss: 0.4012 - val_accuracy: 0.7852 - 492ms/epoch - 7ms/step\n",
      "Epoch 5/100\n",
      "72/72 - 0s - loss: 0.3363 - accuracy: 0.8136 - val_loss: 0.3858 - val_accuracy: 0.7785 - 499ms/epoch - 7ms/step\n",
      "Epoch 6/100\n",
      "72/72 - 0s - loss: 0.3421 - accuracy: 0.8086 - val_loss: 0.3967 - val_accuracy: 0.7785 - 482ms/epoch - 7ms/step\n",
      "Epoch 7/100\n",
      "72/72 - 0s - loss: 0.3414 - accuracy: 0.8047 - val_loss: 0.3843 - val_accuracy: 0.7785 - 495ms/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "72/72 - 0s - loss: 0.3298 - accuracy: 0.8159 - val_loss: 0.4045 - val_accuracy: 0.7696 - 497ms/epoch - 7ms/step\n",
      "Epoch 9/100\n",
      "72/72 - 1s - loss: 0.3271 - accuracy: 0.8198 - val_loss: 0.3772 - val_accuracy: 0.7740 - 536ms/epoch - 7ms/step\n",
      "Epoch 10/100\n",
      "72/72 - 1s - loss: 0.3268 - accuracy: 0.8195 - val_loss: 0.3849 - val_accuracy: 0.7852 - 512ms/epoch - 7ms/step\n",
      "Epoch 11/100\n",
      "72/72 - 1s - loss: 0.3292 - accuracy: 0.8212 - val_loss: 0.4180 - val_accuracy: 0.7875 - 502ms/epoch - 7ms/step\n",
      "Epoch 12/100\n",
      "72/72 - 0s - loss: 0.3320 - accuracy: 0.8044 - val_loss: 0.3770 - val_accuracy: 0.7852 - 487ms/epoch - 7ms/step\n",
      "Epoch 13/100\n",
      "72/72 - 1s - loss: 0.3176 - accuracy: 0.8268 - val_loss: 0.3638 - val_accuracy: 0.7808 - 544ms/epoch - 8ms/step\n",
      "Epoch 14/100\n",
      "72/72 - 1s - loss: 0.3183 - accuracy: 0.8243 - val_loss: 0.4065 - val_accuracy: 0.7562 - 520ms/epoch - 7ms/step\n",
      "Epoch 15/100\n",
      "72/72 - 0s - loss: 0.3217 - accuracy: 0.8142 - val_loss: 0.3752 - val_accuracy: 0.7987 - 497ms/epoch - 7ms/step\n",
      "Epoch 16/100\n",
      "72/72 - 1s - loss: 0.3197 - accuracy: 0.8125 - val_loss: 0.3673 - val_accuracy: 0.7673 - 504ms/epoch - 7ms/step\n",
      "\n",
      "\n",
      "\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.3156 - accuracy: 0.8259\n",
      "\n",
      "Test loss: 0.32. Test accuracy: 82.59%\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu',kernel_initializer='he_uniform'),\n",
    " \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') \n",
    "])\n",
    "\n",
    "\n",
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model3.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "\n",
    "model3.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size=batch_size, \n",
    "          epochs=max_epochs, \n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          verbose = 2 # making sure we get enough information about the training process\n",
    "          )  \n",
    "\n",
    "print('\\n\\n')\n",
    "test_loss, test_accuracy = model3.evaluate(test_inputs, test_targets)\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to 365 Data Science, thats where I learned most of the content I used to create these models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
